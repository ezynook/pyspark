{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9eeea09f",
   "metadata": {},
   "source": [
    "## วิธีติดตั้ง PySpark\n",
    "- Version ที่ติดตั้งกับ pip ต้อง Version เดียวกัน\n",
    "- Version ที่ Trino อ่านได้จะเป็น Version 3.1.3 ขึ้นไป \n",
    "```bash\n",
    "$ yum -y install java-1.8.0-openjdk **CentOS**\n",
    "$ apt install default-jdk scala git -y **Ubuntu**\n",
    "$ wget https://dlcdn.apache.org/spark/spark-3.1.3/spark-3.1.3-bin-hadoop2.7.tgz\n",
    "$ tar xvf spark-3.1.3-bin-hadoop2.7.tgz\n",
    "$ sudo mv spark-3.1.3-bin-hadoop2.7/ /opt/spark \n",
    "$ vim ~/.bashrc\n",
    "$ echo \"export SPARK_HOME=/opt/spark\" >> ~/.bashrc\n",
    "$ echo \"export PATH=$SPARK_HOME:$PATH\" >> ~/.bashrc\n",
    "$ echo \"export PYSPARK_PYTHON=/root/anaconda3/bin/python\" >> ~/.bashrc\n",
    "$ echo \"export SPARK_HOME=/opt/spark\" >> ~/.profile\n",
    "$ echo \"export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\" >> ~/.profile\n",
    "$ echo \"export PYSPARK_PYTHON=/usr/bin/python3\" >> ~/.profile\n",
    "$ source ~/.bashrc\n",
    "$ source ~/.profile\n",
    "$ pip install pyspark==3.1.3\n",
    "```\n",
    "--------------------------\n",
    "## Schema ที่ใช้งานได้\n",
    "```bash\n",
    "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\n",
    "```\n",
    "## หากใช้ไม่ได้ให้ใช้คำสั่งนี้\n",
    "```bash\n",
    "ALTER TABLE potential_plant SET SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe';\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c526e4",
   "metadata": {},
   "source": [
    "### PySpark Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f94912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#แบบที่ 1\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\",\"/users/hive/warehouse/\") \\\n",
    "        .appName('myappname') \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "#แบบที่ 2 ส่วนใหญ่ใช้แบบนี้\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['JAVA_HOME'] = '/usr/local/jdk8u222-b10'\n",
    "os.environ['HADOOP_USER_NAME']='hive'\n",
    "os.environ['PYSPARK_PYTHON'] ='/root/anaconda3/bin/python'\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "#    ('spark.sql.hive.metastore.version', '2.3.9'),\n",
    "     ('spark.driver.maxResultSize', '0'),\n",
    "     ('spark.driver.memory', '2g'),\n",
    "     ('spark.sql.repl.eagerEval.enabled','true'),\n",
    "     ('hive.strict.managed.tables','false'),\n",
    "     ('hive.metastore.uris', 'thrift://nn01.bigdata:9083'),\n",
    "     ('metastore.client.capability.check','false')\n",
    "    ])\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"NookTest\").config(conf=conf).enableHiveSupport().getOrCreate();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272abba7",
   "metadata": {},
   "source": [
    "### Schema Create Parquet Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38777197",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"weeknum\", IntegerType(), True),\n",
    "    StructField(\"province\", StringType(), True),\n",
    "    StructField(\"new_case\", IntegerType(), True),\n",
    "    StructField(\"total_case\", IntegerType(), True),\n",
    "    StructField(\"new_case_excludeabroad\", IntegerType(), True),\n",
    "    StructField(\"total_case_excludeabroad\", IntegerType(), True),\n",
    "    StructField(\"new_death\", IntegerType(), True),\n",
    "    StructField(\"total_death\", IntegerType(), True),\n",
    "    StructField(\"update_date\", TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e4c75",
   "metadata": {},
   "source": [
    "### Get Data From API And Compare With Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"https://covid19.ddc.moph.go.th/api/Cases/today-cases-by-provinces\")\n",
    "df.to_csv('/tmp/tbl_covid_0.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff021ca",
   "metadata": {},
   "source": [
    "### Read File after Pandas to_csv by Sep=\";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e370d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"file:///root/tbl_covid_0.csv\", sep=\";\", index=False, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9707075",
   "metadata": {},
   "source": [
    "### Save To Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"nook.tbl_covid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74eb3a1",
   "metadata": {},
   "source": [
    "### Walk OS Remove File in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aee9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeFile():\n",
    "    os.system(\"hdfs dfs -rm -r /user/hive/warehouse/nook.db/tbl_covid/*\")\n",
    "    \n",
    "removeFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec505cfe",
   "metadata": {},
   "source": [
    "### Walk OS List File in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c55aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listFile():\n",
    "    os.system(\"hdfs dfs -ls /user/hive/warehouse/nook.db/tbl_covid/\")\n",
    "    \n",
    "listFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee19688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from nook.tbl_covid limit 2\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcf324",
   "metadata": {},
   "source": [
    "### Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = spark.sql(\"ALTER TABLE nook.tbl_covid ADD columns (Testcols string)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003de812",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = spark.sql('SELECT * FROM nook.tbl_covid limit 1')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0eaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.write.mode(\"overwrite\").saveAsTable(\"nook.tbl_covid2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a105152",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = spark.sql(\"DROP table nook.tbl_covid_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
