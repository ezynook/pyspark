{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97debf2-b33a-4780-a905-98f4298c2eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abcc64c3",
   "metadata": {},
   "source": [
    "## Spark Context แบบที่ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086fbde-d5be-47ef-bf70-af6eac4bc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.iceberg', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.iceberg.type', 'hadoop')\n",
    "        .set('spark.sql.catalog.iceberg.warehouse', 'hdfs:/user/hive/warehouse/iceberg')\n",
    ")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa76243e",
   "metadata": {},
   "source": [
    "## Spark Context แบบที่ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ac734",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "os.environ['HADOOP_USER_NAME']='hive'\n",
    "os.environ['PYSPARK_PYTHON'] ='/opt/miniconda3/bin/python'\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "     ('spark.driver.maxResultSize', '0'),\n",
    "     ('spark.sql.catalog.iceberg', 'org.apache.iceberg.spark.SparkCatalog'),\n",
    "     ('spark.sql.catalog.iceberg.type', 'hadoop'),\n",
    "     ('spark.sql.catalog.iceberg.warehouse', 'hdfs:/user/hive/warehouse/iceberg'),\n",
    "     ('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178'),\n",
    "     ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
    "     ('spark.driver.memory', '2g'),\n",
    "     ('spark.sql.repl.eagerEval.enabled','true'),\n",
    "     ('hive.strict.managed.tables','false'),\n",
    "     ('hive.metastore.uris', 'thrift://hive-metastore:9083'),\n",
    "     ('metastore.client.capability.check','false')\n",
    "    ])\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Iceberg\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed107dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"weeknum\", StringType(), True),\n",
    "    StructField(\"province\", StringType(), True),\n",
    "    StructField(\"new_case\", IntegerType(), True),\n",
    "    StructField(\"total_case\", IntegerType(), True),\n",
    "    StructField(\"new_case_excludeabroad\", IntegerType(), True),\n",
    "    StructField(\"total_case_excludeabroad\", IntegerType(), True),\n",
    "    StructField(\"new_death\", IntegerType(), True),\n",
    "    StructField(\"total_death\", IntegerType(), True),\n",
    "    StructField(\"update_date\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc007a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.read_json(\"https://covid19.ddc.moph.go.th/api/Cases/today-cases-by-provinces\"), schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddddb52c",
   "metadata": {},
   "source": [
    "## Save To Hive with ICEBERG Type\n",
    "* append\n",
    "* create\n",
    "* createOrReplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeTo(\"iceberg.covid\").using(\"iceberg\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"iceberg\").load(\"hdfs://DEMOHDFS/user/hive/warehouse/iceberg_pyspark/tbl_engineer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a073d09ee1f0fc65c4fdd44cfa2f842cc367f31b284cfeac679ae38e6da887c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
