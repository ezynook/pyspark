{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c8529cf",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/d120d4367f4fcaea0086ec2533ecad35c4ce2fadc313071ee2c26ff319833168/68747470733a2f2f696365626572672e6170616368652e6f72672f646f63732f6c61746573742f696d672f496365626572672d6c6f676f2e706e67\" width=\"250\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "> ### Iceberg architecture\n",
    "\n",
    "<div align=\"center\">\n",
    "\t<img src=\"https://www.dremio.com/wp-content/uploads/2022/08/Fig1-1-768x432.png\">\n",
    "</div>\n",
    "<h4 align=\"right\" style=\"color: blue;\">Author: By Pasit Y.</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a41802",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd766cf",
   "metadata": {},
   "source": [
    "## กำหนด Environment สำหรับ Incerg จะเหมือนกับ PySpark แค่เพิ่มมาบาง Config เท่านั้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ceba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['JAVA_HOME'] = '/usr/local/jdk8u222-b10'\n",
    "os.environ['HADOOP_USER_NAME']='hive'\n",
    "os.environ['PYSPARK_PYTHON'] ='/HDFS01/anaconda3/envs/main/bin/python'\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "     ('spark.driver.maxResultSize', '0'),\n",
    "     ('spark.sql.catalog.iceberg', 'org.apache.iceberg.spark.SparkCatalog'),\n",
    "     ('spark.sql.catalog.iceberg.type', 'hadoop'),\n",
    "     ('spark.sql.catalog.iceberg.warehouse', 'hdfs:/user/hive/warehouse/iceberg_test'),\n",
    "     ('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178'),\n",
    "     ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
    "     ('spark.driver.memory', '2g'),\n",
    "     ('spark.sql.repl.eagerEval.enabled','true'),\n",
    "     #('hive.strict.managed.tables','false'),\n",
    "     ('hive.metastore.uris', 'thrift://nn01.bigdata:9083'),\n",
    "     ('metastore.client.capability.check','false')\n",
    "    ])\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Iceberg\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0350284",
   "metadata": {},
   "source": [
    "## อ่าน CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73479c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF = spark.read.csv(\"file:///HDFS01/airflow/notebooks/Pasit/PySpark Tutorial/example.csv\",\n",
    "                      header=True, inferSchema=True)\n",
    "myDF = myDF.withColumn('age_group', translate('age_group', ' ', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59017d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 align=\"center\">Iceberg Read / Write (PySpark)</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c894fe4",
   "metadata": {},
   "source": [
    "## สร้าง directory ใน HDFS\n",
    "* ```hdfs -dfs -mkdir /iceberg```\n",
    "* ```hdfs -dfs -chmod 777 /iceberg```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f095739",
   "metadata": {},
   "source": [
    "## เขียนข้อมูลลงไปยัง HDFS\n",
    "* แบบที่ 1 คือบันทึกไปยัง HDFS แบบค่า Default\n",
    "* แบบที่ 2 คือกำหนด Partition File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ea90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "myDF.writeTo(\"iceberg_test.test_iceberg3\") \\\n",
    "    .tableProperty(\"table_type\", \"ICEBERG\") \\\n",
    "    .create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "myDF.writeTo(\"iceberg_test.test_iceberg2\") \\\n",
    "    .tableProperty(\"table_type\", \"ICEBERG\") \\\n",
    "    .create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ae659",
   "metadata": {},
   "source": [
    "## อ่านข้อมูล Iceberg จาก HDFS ในรูปแบบของ Schema.tableName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb7a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"iceberg\").load(\"iceberg_test.tbl_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b052f",
   "metadata": {},
   "source": [
    "## อ่านข้อมูล Iceberg จาก HDFS ในรูปแบบของ DFS Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .load(\"/user/hive/warehouse/iceberg_test/example_iceberg_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd220e4",
   "metadata": {},
   "source": [
    "## Insert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f25047",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO iceberg.example (_c0,age_group,vaccine_status,outcome) VALUES (268168, 'xxx','xxx','xxx')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c9a0d",
   "metadata": {},
   "source": [
    "## Update Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ac2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"UPDATE iceberg.example SET vaccine_status = 'vaccinateds' WHERE age_group = 'under50'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c4361",
   "metadata": {},
   "source": [
    "## Delete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DELETE FROM iceberg.example WHERE _c0 = '268168'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9d864",
   "metadata": {},
   "source": [
    "## Snapshot ID ในการกระทำกับข้อมูล CRUD จะแสดงข้อมูลประวัติทั้งหมด โดยสามารถแยก Operation ได้ว่ามีการกระทำ Statement ใด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc235f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg.example.snapshots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6421b95",
   "metadata": {},
   "source": [
    "## แสดงข้อมูลไฟล์ที่ได้เพิ่มข้อมูลเข้าไป จะเป็นชนิด Parquet โดยจะบอกถึง Partiton ทั้งหมด แต่ละไฟล์มี Record, Size เท่าไหร่ และได้ทำการ Split ไว้กี่ไฟล์"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg.example.files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26753b47",
   "metadata": {},
   "source": [
    "## แสดง Snapshot ของไฟล์ Parquet ทั้งหมด พร้อมบอกวันที่และเวลาที่ได้กระทำกับไฟล์นั้นๆ พร้อม Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efead06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            h.made_current_at,\n",
    "            s.operation,\n",
    "            h.snapshot_id,\n",
    "            h.is_current_ancestor,\n",
    "            s.summary['spark.app.id']\n",
    "        FROM\n",
    "        \ticeberg.example.history h\n",
    "        JOIN\n",
    "        \ticeberg.example.snapshots s\n",
    "          \tON h.snapshot_id = s.snapshot_id\n",
    "        ORDER BY\n",
    "            made_current_at\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecede5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg.example.table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34169f",
   "metadata": {},
   "source": [
    "## นอกจากนั้นยังสามารถ Query ระบุ Snapshot ID ได้ดังนี้\n",
    "* ### ระบุแค่ Snapshot ID ที่ต้องการหาข้อมูลที่เจาะจงเท่านั้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read \\\n",
    "    .option(\"snapshot-id\", 10963874102873L) \\\n",
    "    .load(\"/iceberg/default/example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda18262",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 align=\"center\">ปรับใช้ Iceberg กับ TrinoDB</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://github.com/trinodb/trino/raw/master/.github/homepage.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bbd9da",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">** Hive ไม่สามารถอ่าน Iceberg ได้</h3>\n",
    "\n",
    "## เพิ่ม Iceberg Catalog\n",
    "``` vim /home/trino/etc/catalog/iceberg.properties```\n",
    "### จากนั้นเพิ่ม Parameter ดังนี้\n",
    "```bash\n",
    "connector.name=iceberg\n",
    "hive.metastore.uri=thrift://hive-metastore:9083\n",
    "hive.metastore.username=hive\n",
    "iceberg.file-format=PARQUET\n",
    "iceberg.hive-catalog-name=iceberg\n",
    "iceberg.register-table-procedure.enabled=true\n",
    "#SNAPPY, LZ4, ZSTD, GZIP\n",
    "iceberg.compression-codec=ZSTD\n",
    "iceberg.unique-table-location=false\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee97f1a",
   "metadata": {},
   "source": [
    "## สร้าง Schema และ Table สำหรับการเขียน Iceberg\n",
    "> ### การใช้งาน Iceberg โดยใช้ Trino มีความจำเป็นต้อง Mount Fuse เพื่อบันทึกลงไปยัง HDFS (โดย Default จะบันทึกลงไปที่ Linux File system) หากไม่ต้องการ Mount Fuse ก็สามารถเลือก Allow User ผ่าน Ranger ได้เช่นเดียวกัน\n",
    "\n",
    "<h3 style=\"color:green;\">** รันคำสั่งต่อไปนี้บน Trino CLI หรือ DBeaver</h3>\n",
    "<h3 style=\"color: red;\">** หากไม่สามารถสร้างได้ อาจจะติดสิทธิของ HDFS ให้เลือกแก้ไขตามวิธีดังนี้</h3>\n",
    "<li style=\"color:red;\">- Allow User ผ่าน Ranger</li>\n",
    "<li style=\"color:red;\">- เข้าไป chmod 777 ตาม path ที่สร้างไว้</li>\n",
    " <li style=\"color:red;\">- เปลี่ยน User Trino ให้เป็น hdfs หรือ root</li>\n",
    "\n",
    "<hr>\n",
    "<h3 style=\"color: red;\">หลัง Create Schema และ Table แล้ว ให้ไปใช้คำสั่ง chmod 777 ที่ hdfs เพื่อให้ Trino มีสิทธิเขียนไฟล์</h3>\n",
    "\n",
    "### สร้าง Schema\n",
    "---\n",
    "\n",
    "```sh\n",
    "CREATE SCHEMA \n",
    "\tcatalog.schema\n",
    "WITH (\n",
    "\tlocation='hdfs://nn01.bigdata:8020/user/hive/warehouse/schema.db'\n",
    ");\n",
    "```\n",
    "### สร้าง Table\n",
    "---\n",
    "```bash\n",
    "#Normal Table\n",
    "CREATE TABLE catalog.schema.tablename (\n",
    "    _c0 integer,\n",
    "    age_group varchar,\n",
    "    vaccine_status varchar,\n",
    "    outcome varchar\n",
    ")\n",
    "WITH (\n",
    "    format = 'PARQUET',\n",
    "    location = 'hdfs://nn01.bigdata:8020/user/hive/warehouse/schema.db/tablename'\n",
    ");\n",
    "```\n",
    "---\n",
    "\n",
    "```bash\n",
    "#Partition Table\n",
    "CREATE TABLE catalog.schema.tablename (\n",
    "    _c0 integer,\n",
    "    age_group varchar,\n",
    "    vaccine_status varchar,\n",
    "    outcome varchar\n",
    ")\n",
    "WITH (\n",
    "    format = 'PARQUET',\n",
    "    partitioning = ARRAY['outcome', 'vaccine_status'],\n",
    "    sorted_by = ARRAY['_c0'],\n",
    "    location = 'hdfs://nn01.bigdata:8020/user/hive/warehouse/schema.db/tablename'\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62757a3",
   "metadata": {},
   "source": [
    "### Query เพื่อดูประวัติการ Snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0adddf",
   "metadata": {},
   "source": [
    "```bash\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    iceberg.iceberg_test.\"tbl_example$snapshots\"\n",
    "ORDER BY\n",
    "    committed_at DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd1767",
   "metadata": {},
   "source": [
    "### ย้อนกลับไปยัง Snapshot ก่อนหน้าหรือระบุ Snapshot ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfcfe6b",
   "metadata": {},
   "source": [
    "```bash\n",
    "CALL iceberg.system.rollback_to_snapshot('iceberg_test', 'tbl_example', 8954597067493422955)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f4d87c",
   "metadata": {},
   "source": [
    "### Register Table <- เหมือน Hive External Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e93cd",
   "metadata": {},
   "source": [
    "```bash\n",
    "CALL iceberg.system.register_table(\n",
    "    schema_name => 'iceberg_test',\n",
    "    table_name => 'tbl_example',\n",
    "    table_location => 'hdfs://nn01.bigdata:8020/user/hive/warehouse/tbl_example'\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901214e5",
   "metadata": {},
   "source": [
    "### Unregister Table <- เหมือน Hive External Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0267a21",
   "metadata": {},
   "source": [
    "```bash\n",
    "CALL iceberg.system.unregister_table(\n",
    "    schema_name => 'iceberg_test',\n",
    "    table_name => 'tbl_example'\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818c25f",
   "metadata": {},
   "source": [
    "### แสดงรายการ Record หรือ Rows ทั้งหมดในตารางพร้อมระบุเวลาที่เกิด Condition พร้อมบอกว่า ข้อมูล Row นี้อยู่ในไฟล์ parquet ตัวไหน"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e9918",
   "metadata": {},
   "source": [
    "```bash\n",
    "SELECT\n",
    "    *, \n",
    "    \"$path\", \"$file_modified_time\"\n",
    "FROM\n",
    "    iceberg.iceberg_test.tbl_example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb4c86",
   "metadata": {},
   "source": [
    "### Query แบบระบุไฟล์ Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeba87a",
   "metadata": {},
   "source": [
    "```bash\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    iceberg.iceberg_test.tbl_example\n",
    "WHERE\n",
    "    \"$path\" = 'hdfs://nn01.bigdata:8020/user/hive/warehouse/iceberg_test/tbl_example/data/file1.parquet'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
