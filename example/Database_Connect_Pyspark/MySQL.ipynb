{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ad02d7-f469-434b-a160-36ea4ff40603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "276e7c58-753b-453a-9601-bea6da9c520a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 04:09:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/22 04:09:03 WARN DependencyUtils: Local jar /HDFS01/airflow/notebooks/Pasit/Database_Connect_Pyspark/jars/mysql-connector-java-8.0.32.jar does not exist, skipping.\n",
      "23/02/22 04:09:03 INFO SparkContext: Running Spark version 3.3.1\n",
      "23/02/22 04:09:03 INFO ResourceUtils: ==============================================================\n",
      "23/02/22 04:09:03 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/02/22 04:09:03 INFO ResourceUtils: ==============================================================\n",
      "23/02/22 04:09:03 INFO SparkContext: Submitted application: MySQL\n",
      "23/02/22 04:09:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/02/22 04:09:03 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/02/22 04:09:03 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/02/22 04:09:03 INFO SecurityManager: Changing view acls to: root\n",
      "23/02/22 04:09:03 INFO SecurityManager: Changing modify acls to: root\n",
      "23/02/22 04:09:03 INFO SecurityManager: Changing view acls groups to: \n",
      "23/02/22 04:09:03 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/02/22 04:09:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "23/02/22 04:09:04 INFO Utils: Successfully started service 'sparkDriver' on port 33171.\n",
      "23/02/22 04:09:04 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/02/22 04:09:04 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/02/22 04:09:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/02/22 04:09:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/02/22 04:09:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/02/22 04:09:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cacd08ac-324b-45b5-854b-30240ed606af\n",
      "23/02/22 04:09:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "23/02/22 04:09:04 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/02/22 04:09:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/02/22 04:09:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/02/22 04:09:05 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/02/22 04:09:05 INFO Utils: Successfully started service 'SparkUI' on port 4043.\n",
      "23/02/22 04:09:05 ERROR SparkContext: Failed to add jars/mysql-connector-java-8.0.32.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /HDFS01/airflow/notebooks/Pasit/Database_Connect_Pyspark/jars/mysql-connector-java-8.0.32.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/22 04:09:05 INFO Executor: Starting executor ID driver on host nn02.bigdata\n",
      "23/02/22 04:09:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/02/22 04:09:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41560.\n",
      "23/02/22 04:09:05 INFO NettyBlockTransferService: Server created on nn02.bigdata:41560\n",
      "23/02/22 04:09:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/02/22 04:09:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, nn02.bigdata, 41560, None)\n",
      "23/02/22 04:09:05 INFO BlockManagerMasterEndpoint: Registering block manager nn02.bigdata:41560 with 366.3 MiB RAM, BlockManagerId(driver, nn02.bigdata, 41560, None)\n",
      "23/02/22 04:09:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, nn02.bigdata, 41560, None)\n",
      "23/02/22 04:09:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, nn02.bigdata, 41560, None)\n"
     ]
    }
   ],
   "source": [
    "#MySQL\n",
    "spark = SparkSession.builder \\\n",
    "           .appName('MySQL') \\\n",
    "           .config(\"spark.jars\", \"jars/mysql-connector-java-8.0.32.jar\") \\\n",
    "           .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05582160-5682-4ee8-8d77-ba5bb1b9bd31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 04:09:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/02/22 04:09:06 INFO SharedState: Warehouse path is 'file:/HDFS01/airflow/notebooks/Pasit/Database_Connect_Pyspark/spark-warehouse'.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://192.168.10.22:3306/test\") \\\n",
    "    .option(\"query\", \"select * from data limit 5\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"P@ssw0rd\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccf96e0-9b74-4d0f-80e2-27af9c4ece8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 04:09:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/02/22 04:09:13 INFO CodeGenerator: Code generated in 357.03888 ms\n",
      "23/02/22 04:09:13 INFO SparkContext: Starting job: toPandas at /tmp/ipykernel_11895/3523621387.py:1\n",
      "23/02/22 04:09:13 INFO DAGScheduler: Got job 0 (toPandas at /tmp/ipykernel_11895/3523621387.py:1) with 1 output partitions\n",
      "23/02/22 04:09:13 INFO DAGScheduler: Final stage: ResultStage 0 (toPandas at /tmp/ipykernel_11895/3523621387.py:1)\n",
      "23/02/22 04:09:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/02/22 04:09:13 INFO DAGScheduler: Missing parents: List()\n",
      "23/02/22 04:09:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at toPandas at /tmp/ipykernel_11895/3523621387.py:1), which has no missing parents\n",
      "23/02/22 04:09:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 18.2 KiB, free 366.3 MiB)\n",
      "23/02/22 04:09:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 366.3 MiB)\n",
      "23/02/22 04:09:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on nn02.bigdata:41560 (size: 7.2 KiB, free: 366.3 MiB)\n",
      "23/02/22 04:09:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "23/02/22 04:09:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at toPandas at /tmp/ipykernel_11895/3523621387.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "23/02/22 04:09:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/02/22 04:09:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (nn02.bigdata, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "23/02/22 04:09:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 04:09:14 INFO JDBCRDD: closed connection\n",
      "23/02/22 04:09:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2469 bytes result sent to driver\n",
      "23/02/22 04:09:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 603 ms on nn02.bigdata (executor driver) (1/1)\n",
      "23/02/22 04:09:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/02/22 04:09:14 INFO DAGScheduler: ResultStage 0 (toPandas at /tmp/ipykernel_11895/3523621387.py:1) finished in 0.893 s\n",
      "23/02/22 04:09:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/02/22 04:09:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/02/22 04:09:14 INFO DAGScheduler: Job 0 finished: toPandas at /tmp/ipykernel_11895/3523621387.py:1, took 0.970229 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>dag_id</th>\n",
       "      <th>run_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>state</th>\n",
       "      <th>try_number</th>\n",
       "      <th>hostname</th>\n",
       "      <th>unixname</th>\n",
       "      <th>...</th>\n",
       "      <th>pid</th>\n",
       "      <th>max_tries</th>\n",
       "      <th>executor_config</th>\n",
       "      <th>pool_slots</th>\n",
       "      <th>queued_by_job_id</th>\n",
       "      <th>external_executor_id</th>\n",
       "      <th>trigger_id</th>\n",
       "      <th>trigger_timeout</th>\n",
       "      <th>next_method</th>\n",
       "      <th>next_kwargs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LoadtoHive</td>\n",
       "      <td>telemetering_stations</td>\n",
       "      <td>scheduled__2022-08-30T10:00:00+00:00</td>\n",
       "      <td>30/8/2022 10:15:49.431311+00</td>\n",
       "      <td>30/8/2022 10:15:51.893139+00</td>\n",
       "      <td>2.461828</td>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "      <td>Airflow1</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>71441.0</td>\n",
       "      <td>1</td>\n",
       "      <td>gAV9lC4=</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RemoveTmp</td>\n",
       "      <td>early_warning_status</td>\n",
       "      <td>scheduled__2022-09-20T09:18:00+00:00</td>\n",
       "      <td>20/9/2022 09:33:20.524631+00</td>\n",
       "      <td>20/9/2022 09:33:20.524631+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>skipped</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>gAV9lC4=</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>backup_data_schema</td>\n",
       "      <td>hive_data_backup</td>\n",
       "      <td>scheduled__2022-08-29T10:00:00+00:00</td>\n",
       "      <td>30/8/2022 10:00:01.537461+00</td>\n",
       "      <td>30/8/2022 10:16:09.055564+00</td>\n",
       "      <td>967.518103</td>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "      <td>Airflow1</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>240326.0</td>\n",
       "      <td>1</td>\n",
       "      <td>gAV9lC4=</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pythonChagetoCSV</td>\n",
       "      <td>early_warning_status</td>\n",
       "      <td>scheduled__2022-08-30T10:03:00+00:00</td>\n",
       "      <td>30/8/2022 10:18:15.738998+00</td>\n",
       "      <td>30/8/2022 10:18:18.098593+00</td>\n",
       "      <td>2.359595</td>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "      <td>Airflow1</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>83335.0</td>\n",
       "      <td>1</td>\n",
       "      <td>gAV9lC4=</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>log_datetime</td>\n",
       "      <td>hive_data_backup</td>\n",
       "      <td>scheduled__2022-08-29T10:00:00+00:00</td>\n",
       "      <td>30/8/2022 10:16:09.923257+00</td>\n",
       "      <td>30/8/2022 10:16:10.09634+00</td>\n",
       "      <td>0.173083</td>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "      <td>Airflow1</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>73119.0</td>\n",
       "      <td>1</td>\n",
       "      <td>gAV9lC4=</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              task_id                 dag_id  \\\n",
       "0          LoadtoHive  telemetering_stations   \n",
       "1           RemoveTmp   early_warning_status   \n",
       "2  backup_data_schema       hive_data_backup   \n",
       "3    pythonChagetoCSV   early_warning_status   \n",
       "4        log_datetime       hive_data_backup   \n",
       "\n",
       "                                 run_id                    start_date  \\\n",
       "0  scheduled__2022-08-30T10:00:00+00:00  30/8/2022 10:15:49.431311+00   \n",
       "1  scheduled__2022-09-20T09:18:00+00:00  20/9/2022 09:33:20.524631+00   \n",
       "2  scheduled__2022-08-29T10:00:00+00:00  30/8/2022 10:00:01.537461+00   \n",
       "3  scheduled__2022-08-30T10:03:00+00:00  30/8/2022 10:18:15.738998+00   \n",
       "4  scheduled__2022-08-29T10:00:00+00:00  30/8/2022 10:16:09.923257+00   \n",
       "\n",
       "                       end_date    duration    state  try_number  hostname  \\\n",
       "0  30/8/2022 10:15:51.893139+00    2.461828  success           1  Airflow1   \n",
       "1  20/9/2022 09:33:20.524631+00    0.000000  skipped           0             \n",
       "2  30/8/2022 10:16:09.055564+00  967.518103  success           1  Airflow1   \n",
       "3  30/8/2022 10:18:18.098593+00    2.359595  success           1  Airflow1   \n",
       "4   30/8/2022 10:16:10.09634+00    0.173083  success           1  Airflow1   \n",
       "\n",
       "  unixname  ...       pid max_tries executor_config  pool_slots  \\\n",
       "0     root  ...   71441.0         1        gAV9lC4=           1   \n",
       "1     root  ...       NaN         1        gAV9lC4=           1   \n",
       "2     root  ...  240326.0         1        gAV9lC4=           1   \n",
       "3     root  ...   83335.0         1        gAV9lC4=           1   \n",
       "4     root  ...   73119.0         1        gAV9lC4=           1   \n",
       "\n",
       "  queued_by_job_id external_executor_id  trigger_id  trigger_timeout  \\\n",
       "0              2.0                                                     \n",
       "1              NaN                                                     \n",
       "2              2.0                                                     \n",
       "3              2.0                                                     \n",
       "4              2.0                                                     \n",
       "\n",
       "  next_method  next_kwargs  \n",
       "0                     null  \n",
       "1                     null  \n",
       "2                     null  \n",
       "3                     null  \n",
       "4                     null  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ba0d2-6eac-4123-b39a-a09d70680f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .option(\"url\", \"jdbc:mysql://192.168.10.22:3306/test\") \\\n",
    "  .option(\"dbtable\", \"engineer\") \\\n",
    "  .option(\"user\", \"root\") \\\n",
    "  .option(\"password\", \"P@ssw0rd\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09191483-2c1f-4ad8-8faa-64976fc92975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
