{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3e25c7-d4e4-44b2-9a71-a75a99f9df35",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/5535944a613e60c9be4d3a96e3d9bd34e5aba5cddc1aa6c6153123a958698289/68747470733a2f2f646f63732e64656c74612e696f2f6c61746573742f5f7374617469632f64656c74612d6c616b652d77686974652e706e67\" width=\"150\">\n",
    "\n",
    "---\n",
    "<h2 align=\"center\">Dalta Lake Documentation</h2>\n",
    "<h5 align=\"right\">Auther: Pasit Y.</h5>\n",
    "\n",
    "---\n",
    "<h3 style=\"color: red;\">\n",
    "    ** Hive ไม่สามารถอ่าน Delta Snapshot ได้ แต่สามารถอ่าน ไฟล์ parquet โดยไม่สนใจ checkpoint โดยการสร้าง schema ดังนี้\n",
    "</h3>\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE `delta_lake.example_table` (\n",
    "  `id` string, \n",
    "  `name` string\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '/path/to/delta/table';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ab9a2a",
   "metadata": {},
   "source": [
    "# Delta PySpark\n",
    "<b>Link สำหรับการติดตั้งและ Document</b>\n",
    "\n",
    "```https://docs.delta.io/```\n",
    "\n",
    "```https://blog.devgenius.io/pyspark-setup-delta-lake-971e2e37330d```\n",
    "\n",
    "```https://kontext.tech/article/1175/introduction-to-delta-lake-with-pyspark```\n",
    "\n",
    "---\n",
    "# Pyspark Config (เพิ่มจากเดิมที่มีอยู่แล้ว)\n",
    "\n",
    "```bash\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "     ('spark.jars.packages', 'io.delta:delta-core_2.12:2.2.0'),\n",
    "     ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
    "     ('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog'),\n",
    "     ('spark.sql.warehouse.dir', '/user/hive/warehouse/delta_lake/'),\n",
    "     ('spark.databricks.delta.replaceWhere.constraintCheck.enabled', 'false'),\n",
    "     ('spark.databricks.delta.replaceWhere.dataColumns.enabled', 'true'),\n",
    "     ('spark.databricks.delta.schema.autoMerge.enabled','true'),\n",
    "    ])\n",
    "```\n",
    "---\n",
    "# TrinoDB Catalog\n",
    "```bash\n",
    "connector.name=delta-lake\n",
    "hive.metastore.uri=thrift://192.168.10.40:9083\n",
    "delta.compression-codec=SNAPPY\n",
    "delta.max-partitions-per-writer=100\n",
    "delta.enable-non-concurrent-writes=true\n",
    "```\n",
    "### Register existing table\n",
    "```sql\n",
    "CALL delta_lake.system.register_table(\n",
    "    \tschema_name => 'delta_lake',\n",
    "    \ttable_name => 'tableName',\n",
    "    \ttable_location => 'hdfs://HDFS/user/hive/warehouse/delta_lake/tableName'\n",
    ")\n",
    "```\n",
    "### Optimize VACUUM\n",
    "```sql\n",
    "CALL delta_lake.system.vacuum('delta_lake', 'tableName', '7d');\n",
    "```\n",
    "---\n",
    "# PrestoDB Catalog\n",
    "```bash\n",
    "connector.name=delta\n",
    "hive.metastore.uri=thrift://192.168.10.40:9083\n",
    "```\n",
    "### การสร้าง Table External เพื่อ Link ไปยัง Delta Lake Existing\n",
    "```sql\n",
    "CREATE TABLE\n",
    "\tdelta_lake.delta_lake.tableName (\n",
    "        col_name INT\n",
    ")\n",
    "WITH (external_location = 'hdfs://HDFS/user/hive/warehouse/delta_lake/tableName');\n",
    "```\n",
    "### ฟังค์ชั่นนี้สามารถใช้งานได้แค่ PrestoDB เท่านั้น (Query Directory Path)\n",
    "```sql\n",
    "SELECT\n",
    "\t*\n",
    "FROM\n",
    "\tdelta_lake.\"$path$\".\"hdfs://HDFS/user/hive/warehouse/delta_lake/tableName\"\n",
    "LIMIT 200;\n",
    "--Query แบบเลือก Snapshot version\n",
    "SELECT * FROM delta_lake.delta_lake.\"tableName@v4\" LIMIT 200;\n",
    "```\n",
    "---\n",
    "# Config /etc/hosts\n",
    "<b>ใส่ Nameservice ของ hadoop ที่เป็น Active namenode สามารถเข้าไปดูได้ที่ core-site.xml</b><br>\n",
    "<b style=\"color: red;\">** ตรวจสอบให้ถูกต้องว่า node ไหน active จะมีผลกับการเข้าไปอ่านใน metastore</b>\n",
    "\n",
    "<b style=\"color: red;\">HDFSSOFTNIX</b>\n",
    "\n",
    "```py\n",
    "192.168.10.40   nn01.bigdata HDFSSOFTNIX\n",
    "192.168.10.41   nn02.bigdata\n",
    "192.168.10.42   dn01.bigdata\n",
    "192.168.10.43   dn02.bigdata\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742178a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from delta import *\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['JAVA_HOME'] = '/usr/local/jdk8u222-b10'\n",
    "os.environ['HADOOP_USER_NAME']='hive'\n",
    "os.environ['PYSPARK_PYTHON'] ='/HDFS01/anaconda3/envs/main/bin/python'\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "     ('spark.driver.maxResultSize', '0'),\n",
    "     ('spark.jars.packages', 'io.delta:delta-core_2.12:2.2.0'),\n",
    "     ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
    "     ('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog'),\n",
    "     ('spark.sql.warehouse.dir', '/user/hive/warehouse/delta_lake/'),\n",
    "     ('spark.databricks.delta.replaceWhere.constraintCheck.enabled', 'false'),\n",
    "     ('spark.databricks.delta.replaceWhere.dataColumns.enabled', 'true'),\n",
    "     ('spark.databricks.delta.schema.autoMerge.enabled','true'),\n",
    "     ('spark.databricks.io.cache.enabled','true'),\n",
    "     ('spark.driver.memory', '2g'),\n",
    "     ('spark.sql.repl.eagerEval.enabled','true'),\n",
    "     ('hive.strict.managed.tables','false'),\n",
    "     ('hive.metastore.uris', 'thrift://nn01.bigdata:9083'),\n",
    "     ('metastore.client.capability.check','false')\n",
    "    ])\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"myApp\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8621344",
   "metadata": {},
   "source": [
    "## Read CSV and Infer Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"file:///HDFS01/airflow/notebooks/Pasit/PySpark Tutorial/round-1to2-line-lists.csv\",\n",
    "                      header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4d8a4",
   "metadata": {},
   "source": [
    "## เขียนลง HDFS Delta Type\n",
    "* .option(\"replaceWhere\", \"start_date >= '2017-01-01' AND end_date <= '2017-01-31'\") คือเขียนทับหากตรงเงื่อนไข Where\n",
    "\n",
    "* .option(\"maxRecordsPerFile\", \"10000\") คือ จำกัด Rows ในการเขียนลง HDFS สามารถ static ใน Spark Config ได้ตังนี้ spark.sql.files.maxRecordsPerFile=Number_of_rows\n",
    "\n",
    "* .option(\"overwriteSchema\", \"true\") คือ สามารถเปลี่ยนประเภทหรือชื่อของคอลัมน์ หรือลบคอลัมน์โดยการเขียนตารางใหม่\n",
    "\n",
    "* .partitionBy(<your-partition-columns>) คือการสร้าง Partition โดยการใส่ชื่อ Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf6db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", True) \\\n",
    "\t.partitionBy(\"province\") \\\n",
    "    .saveAsTable(\"delta_lake.covid_summery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe997d",
   "metadata": {},
   "source": [
    "## เปลี่ยนแปลงชื่อ Column โดยใช้วิธีอ่านแล้ว Overwrite กลับไปยัง table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table(...) \\\n",
    "  .withColumnRenamed(\"dateOfBirth\", \"birthDate\") \\\n",
    "  .write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"overwriteSchema\", \"true\") \\\n",
    "  .saveAsTable(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca038f4b",
   "metadata": {},
   "source": [
    "## อ่าน Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab598760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/user/hive/warehouse/delta_lake/navy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee90b5",
   "metadata": {},
   "source": [
    "## เปิด Transaction เพื่อสามารถ Update, Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd88c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DeltaTable.forPath(spark, \"/user/hive/warehouse/delta_lake/navy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059c45c-6e9c-4e3a-b196-712f7c1f532b",
   "metadata": {},
   "source": [
    "## Update Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.update(\n",
    "    \"id = '1'\",{ \"id\": \"'2909'\" }\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d2961-864b-48eb-ac1f-431c439b0df0",
   "metadata": {},
   "source": [
    "## Delete Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341955c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.delete(\"name = 'Name'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb12ffc",
   "metadata": {},
   "source": [
    "## สร้าง Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033fee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.create(spark) \\\n",
    "  .tableName(\"default.people10m\") \\\n",
    "  .addColumn(\"id\", \"INT\") \\\n",
    "  .addColumn(\"firstName\", \"STRING\") \\\n",
    "  .addColumn(\"middleName\", \"STRING\") \\\n",
    "  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n",
    "  .addColumn(\"gender\", \"STRING\") \\\n",
    "  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n",
    "  .addColumn(\"ssn\", \"STRING\") \\\n",
    "  .addColumn(\"salary\", \"INT\") \\\n",
    "  .partitionedBy(\"gender\") \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f662f",
   "metadata": {},
   "source": [
    "## แสดง Version ของ Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc069e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.history() #ถ้าระบุเลขใน () จะเป็นรูปแบบการ Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64540350",
   "metadata": {},
   "source": [
    "## Restore ไปยัง Version ที่ต้องการ\n",
    "\n",
    "<b style=\"color: red;\">restore จะใช้งานไม่ได้ใน PrestoDB แต่ถ้า UPDATE INSERT DELETE ปกติในรูปแบบ Append จะยังใช้งานได้</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.restoreToVersion(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5671a4",
   "metadata": {},
   "source": [
    "## Optimize Table ปรับโครงสร้างไฟล์ใหม่\n",
    "\n",
    "* แบบ SQL Statement\n",
    "* แบบ Spark Build-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "#spark.sql(\"OPTIMIZE delta_lake.delta\")\n",
    "#spark.sql(\"VACUUM delta_lake.delta\")\n",
    "\n",
    "#2\n",
    "deltaTable = DeltaTable.forName(spark, \"delta_lake.delta_stable\")\n",
    "deltaTable.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac6f12",
   "metadata": {},
   "source": [
    "## ใช้ Spark Read Table แบบระบุ Checkpoint version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1dd1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "          .format(\"delta\") \\\n",
    "          .option(\"versionAsOf\", 1) \\\n",
    "          .load(\"/user/hive/warehouse/delta_lake/navy\")\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
