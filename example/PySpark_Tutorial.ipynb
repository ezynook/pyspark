{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e16ccc",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">การใช้งาน Spark ร่วมกับ Python</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://blog.datath.com/wp-content/uploads/2021/03/Pyspark.png\" align=\"center\" width=\"400\">\n",
    "\n",
    "<h3 align=\"right\" style=\"color:blue;\">Author: Pasit Y.</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16af16e",
   "metadata": {},
   "source": [
    "> ## Import Libraly ที่ต้องการใช้งาน (Spark, Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8bef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb96b625",
   "metadata": {},
   "source": [
    "> ## ตั้งค่า Environment ที่จำเป็นลักษณะมีประมาณนี้โดยปกติจะใช้ได้เครื่อง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a194c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['JAVA_HOME'] = '/usr/local/jdk8u222-b10'\n",
    "os.environ['HADOOP_USER_NAME']='hive'\n",
    "os.environ['PYSPARK_PYTHON'] ='/HDFS01/anaconda3/envs/main/bin/python'\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "     ('spark.driver.maxResultSize', '0'),\n",
    "     ('spark.driver.memory', '4g'),\n",
    "     ('spark.sql.repl.eagerEval.enabled','true'),\n",
    "     #('spark.sql.warehouse.dir\", \"/user/hive/warehouse'),\n",
    "     ('hive.strict.managed.tables','false'),\n",
    "     ('hive.metastore.uris', 'thrift://nn01.bigdata:9083'),\n",
    "     ('metastore.client.capability.check','false')\n",
    "    ])\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"myApp\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb242f",
   "metadata": {},
   "source": [
    "> ## อ่าน CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF = spark.read.csv(\"file:///HDFS01/airflow/notebooks/Pasit/PySpark Tutorial/example.csv\",\n",
    "                      header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd9783",
   "metadata": {},
   "source": [
    "> ## อ่าน Json From URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://covid19.ddc.moph.go.th/api/Cases/today-cases-by-provinces'\n",
    "jsonData = urlopen(url).read().decode('utf-8')\n",
    "rdd = spark.sparkContext.parallelize([jsonData])\n",
    "df = spark.read.json(rdd)\n",
    "df2 = spark.read.json(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebe6ca",
   "metadata": {},
   "source": [
    "> ## สร้าง Cache View Table (Lazy) เพื่อมาทำการ Cleansing จะเร็วกว่า Inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718da0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.createOrReplaceTempView('covid_temp')\n",
    "spark.sql('SELECT * FROM covid_temp').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6977a0",
   "metadata": {},
   "source": [
    "> ## ตัวอย่างการ Filter แต่ละ Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3fde2",
   "metadata": {},
   "source": [
    "### แสดง Schema ของ Datasets หาก Spark ทำการ Create auto ให้แล้วไม่ตรงกับรูปแบบที่เราต้องการ มีความจำเป็นต้อง Manual Create ดังนี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#แสดง Schema ของ Datasets หาก Spark ทำการ Create auto ให้แล้วไม่ตรงกับรูปแบบที่เราต้องการ มีความจำเป็นต้อง Manual Create ดังนี้\n",
    "#schema = StructType([\n",
    "#    StructField(\"year\", LongType(), True),\n",
    "#    StructField(\"weeknum\", LongType(), True),\n",
    "#    StructField(\"province\", StringType(), True),\n",
    "#    StructField(\"new_case\", LongType(), True),\n",
    "#    StructField(\"new_case_excludeabroad\", LongType(), True),\n",
    "#    StructField(\"new_death\", LongType(), True),\n",
    "#    StructField(\"total_case\", LongType(), True),\n",
    "#    StructField(\"total_case_excludeabroad\", LongType(), True),\n",
    "#    StructField(\"total_death\", LongType(), True),\n",
    "#    StructField(\"update_date\", StringType(), True)\n",
    "#)]\n",
    "myDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb92c4",
   "metadata": {},
   "source": [
    "### แสดงข้อมูลแบบเลือก Columns และ Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c83e73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(['year','province']).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b503d",
   "metadata": {},
   "source": [
    "### ลบ Column ที่ไม่ได้ใช้งาน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac56e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('total_death','weeknum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf79b08",
   "metadata": {},
   "source": [
    "### Filter เลือกข้อมูล (Where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f325c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.filter(df.province == 'ระยอง').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8bb52a",
   "metadata": {},
   "source": [
    "### Group By same SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580302dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('province').count().limit(100).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634407c1",
   "metadata": {},
   "source": [
    "### Filter แบบหลายเงื่อนไข (&, |)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d822056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('province','update_date', 'new_case', 'total_case') \\\n",
    "    .filter( (df.province  == \"ระยอง\") | (df.province  == \"ระนอง\") ) \\\n",
    "    .limit(10) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef62964",
   "metadata": {},
   "source": [
    "### To Date Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"update_date\",to_date(col(\"update_date\"))) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787239c",
   "metadata": {},
   "source": [
    "### รวม Column เป็น Column ใหม่ด้วย Sep (|,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a58be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.withColumn(\"update_date\",expr(\" update_date ||'-'|| update_date\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dfc3f6",
   "metadata": {},
   "source": [
    "### Case When Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8046f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\"province\", expr(\"\"\"CASE\n",
    "\t\t\t\t\t\t\t\t\t\t\tWHEN province = 'ระยอง' \n",
    "                \t\t\t\t\t\t\t\tTHEN 'Pattani'\n",
    "           \t\t\t\t\t\t\t\t\tWHEN province = 'กรุงเทพมหานคร'\n",
    "                                            \tTHEN 'Bangkok'\n",
    "                                             ELSE 'unknown' END\n",
    "           \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d16d1",
   "metadata": {},
   "source": [
    "### แทนที่ Special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"details\", regexp_replace(\"details\", \"\\r\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f971f",
   "metadata": {},
   "source": [
    "### สร้าง Columns ใหม่แบบมีเงื่อนไข"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('new_cols', lit(None).cast(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e129c8d",
   "metadata": {},
   "source": [
    "### เปลี่ยนชื่อ Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"new_case\",\"newcase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c3dede",
   "metadata": {},
   "source": [
    "### ลบ Columns ที่ไม่ต้องการใช้ออก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c3af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"province\", \"new_case\", \"update_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9087d",
   "metadata": {},
   "source": [
    "### ลบค่าที่ซ้ำกัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe23cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d128aaea",
   "metadata": {},
   "source": [
    "### แทนที่ค่าว่างด้วย Assign Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763cc37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.na.fill(value='ว่าง',subset=[\"new_cols\"]).show()\n",
    "\n",
    "#df.fillna(value=0)\n",
    "\n",
    "df.na.fill('ไม่มีข้อมูล', 'new_cols')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b58654",
   "metadata": {},
   "source": [
    "### Join 2 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611aad12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3 = df.join(df2,\n",
    "               df.province == df2.province,\n",
    "               \"inner\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ec7ea",
   "metadata": {},
   "source": [
    "### Order By & Sort By\n",
    "- OrderBy ASC .asc()\n",
    "- OrderBy DESC .desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120eea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.sort(df.total_death.desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca02e17",
   "metadata": {},
   "source": [
    "### รวม Dataframe (Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unionDF = df.union(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e5847",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb167c1",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">HDFS Session</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11955a9f",
   "metadata": {},
   "source": [
    "# เขียนข้อมูลลงไป HDFS\n",
    "- <h3 style=\"color: red;\"> 1. Basic Save ไม่ได้กำหนดค่าอะไรค่าเริ่มต้นจะเป็น PARQUET Type</h3>\n",
    "- ### <h3 style=\"color: blue;\"> 2. Format Static กำหนดค่ารูปแบบที่จะไปจัดเก็บใน HDFS มี 2 รูปแบบคือ PARQUET, ORC </h3>\n",
    "- ###  <h3 style=\"color: green;\">3. กำหนดรูปแบบการเก็บแบบ Partition รูปแบบการเก็บแบบแยก schema/tablename/partiton ข้อดีคือจะมีความรวดเร็วในการ Insert เนื่องจากรูปแบบการเก็บชัดเจนหาก Data มาในชนิดเดียวกัน แต่หากมีการเก็บข้อมูลที่ไม่เหมือนกันหรือต่าง Data Source กัน อาจจะต้องการ Repartition ก่อนการ Save เพื่อเพิ่มหรือลดพาร์ติชัน RDD/DataFrame แต่ถ้าต้องการลดเท่านั้นให้ใช้ coalesce() เพื่อลดจำนวนพาร์ติชันอย่างมีประสิทธิภาพ </h3>\n",
    "- ### <h3 style=\"color: #D35400;\">4. กำหนดรูปแบบการเก็บลักษณะ Partition -> จำนวนไฟล์ที่ Split ออกมาในลักษณะ schema/tablename/partiton/bucket (ตามจำนวน bucket ที่ระบุไว้) เพื่อให้มีขนาดไฟล์ที่เล็กเพื่อความเร็วในการ Query </h3>\n",
    "---\n",
    "# ข้อดีและข้อเสีย\n",
    "- ### PartitonBy \n",
    "  * ### ข้อดีคือ มีความเร็วในการจัดเก็บข้อมูลสูงทั้ง Overwrite / Append เน้นเขียนซ้ำเพิ่มลดจำนวนบ่อยครั้ง (ETL High Performance)\n",
    "  * ### ข้อเสียคือ ความเร็วในการ Query ยังสู้ BucketBy ไม่ได้ เนื่องจาก Partiton มันคือการ Group เพื่อให้มีการลดความยุ่งยากค้นหาของ HDFS (MapReduce)\n",
    "- ### BucketBy\n",
    "  * ### ข้อดีคือ มีการ Query ที่รวดเร็วกว่า PartitionBy เนื่องจาก Bucket จะทำการ Split Large File ให้เป็นไฟล์เล็กๆหลายๆไฟล์ ทำให้การ Reduce ข้อมูลได้มีความรวดเร็วมากกว่า\n",
    "  * ### ข้อเสียคือ ไม่เหมาะกับข้อมูลที่เป็นรูปแบบ Incremental หรือมีข้อมูลเข้ามาตลอดเวลาและ Overwrite / Append ช้ากว่า PartitionBy\n",
    "---\n",
    "# MapReduce คืออะไร\n",
    "\n",
    "> ## แยกเป็นรูปแบบการทำงานดังนี้\n",
    "* ### Map คือ การเอาข้อมูลขนาดใหญ่มาจัดให้อยู่ในรูปแบบของ Key => Value หลังจากนั้นมาทำการ Shuffle ให้กระจายออกมาเป็นหลายๆไฟล์ และการะจายการจัดเก็บไปยัง Worker ต่างๆใน Cluster\n",
    "* ### Reduce ตือการนำข้อมูลที่ถูก Map Key => Value และ Shuffle จากหลายๆเครื่องมารวมกันเพื่อให้ได้มาเป็น Object 1 ก้อน เมื่อเวลาเรา Query ข้อมูล ดังภาพ\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*WlJFdJP108MTvR8j-S0UGA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc924ca3",
   "metadata": {},
   "source": [
    "### อ่านด้วย read.table ให้ระบุเป็น schema.tablename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF = spark.read.table(\"pyspark.j64_ew_status_ability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad13511f",
   "metadata": {},
   "source": [
    "### อ่านข้อมูลโดยใช้ SQL Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c0dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF2 = spark.sql(\"SELECT * FROM pyspark.j64_ew_status_ability LIMIT 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d48d71",
   "metadata": {},
   "source": [
    "### อ่านข้อมูลแบบใช้ Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF3 = spark.read.table(\"pyspark.j64_ew_status_ability\") \\\n",
    "       .filter( (myDF.Group_ew  == \"Mobile\") | (myDF.Status  == \"100\") ) \\\n",
    "       .limit(10) \\\n",
    "       .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b1b3e",
   "metadata": {},
   "source": [
    "<h3 style=\"color: red;\"> เขียนข้อมูลไปยัง HDFS (ข้อที่ 1)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d35eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"pyspark.covid_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9477bb6",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue;\"> เขียนข้อมูลไปยัง HDFS (ข้อที่ 2)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ea598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "df.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"pyspark.covid_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd2de97",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green;\"> เขียนข้อมูลไปยัง HDFS (ข้อที่ 3)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "df = df.repartition(6) #ตัวเลขที่ต้องการเพิ่ม/ลด จำนวน Partition\n",
    "df = df.coalesce(4) ##ตัวเลขที่ต้องการลด จำนวน Partition\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"province\") \\\n",
    "    .saveAsTable(\"pyspark.covid_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708de447",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #D35400;\"> เขียนข้อมูลไปยัง HDFS (ข้อที่ 4)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "df = df.repartition(6) #ตัวเลขที่ต้องการเพิ่ม/ลด จำนวน Partition\n",
    "df = df.coalesce(4) ##ตัวเลขที่ต้องการลด จำนวน Partition\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"province\") \\\n",
    "    .bucketBy(10, \"year\")\n",
    "    .saveAsTable(\"pyspark.covid_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df11cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "myDF.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"pyspark.covid_temp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc703d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('sudo -u hdfs hdfs dfs -ls /user/hive/warehouse/iceberg_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c4122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
